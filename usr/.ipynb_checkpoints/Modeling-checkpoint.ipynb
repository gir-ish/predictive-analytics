{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb0733a8-53a9-4e5a-8bb1-1fc1ff9022c4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# -----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b938b6b1-0285-4510-9b3a-a0d9cb5d77cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c2efef6-31ab-47cb-a066-76a20b9c01d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev=pd.read_csv('/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/WavLM-EMOFAKE/Chinese_dev_WavLM.csv')\n",
    "test=pd.read_csv('/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/WavLM-EMOFAKE/Chinese_test_WavLM.csv')\n",
    "train=pd.read_csv('/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/WavLM-EMOFAKE/Chinese_train_WavLM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f331959-2018-478d-8a36-419204372f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['filename', '0', '1', '2', '3', '4', '5', '6', '7', '8',\n",
       "       ...\n",
       "       '759', '760', '761', '762', '763', '764', '765', '766', '767', 'class'],\n",
       "      dtype='object', length=770)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59739a80-1d9a-4f2f-b1bd-5b5e79323eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['filename', '0', '1', '2', '3', '4', '5', '6', '7', '8',\n",
       "       ...\n",
       "       '759', '760', '761', '762', '763', '764', '765', '766', '767', 'class'],\n",
       "      dtype='object', length=770)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32e1f085-3efd-44fe-93a4-d9354e521e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['filename', '0', '1', '2', '3', '4', '5', '6', '7', '8',\n",
       "       ...\n",
       "       '759', '760', '761', '762', '763', '764', '765', '766', '767', 'class'],\n",
       "      dtype='object', length=770)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6af30080-1d84-4094-ac91-262931fe8760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S2_Surprise2Angry_0010_001420.wav</td>\n",
       "      <td>0.189128</td>\n",
       "      <td>-0.209342</td>\n",
       "      <td>-0.004541</td>\n",
       "      <td>0.491101</td>\n",
       "      <td>-0.249494</td>\n",
       "      <td>-0.100917</td>\n",
       "      <td>0.049525</td>\n",
       "      <td>0.006354</td>\n",
       "      <td>-0.017165</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.222937</td>\n",
       "      <td>-0.228415</td>\n",
       "      <td>0.339616</td>\n",
       "      <td>-0.211259</td>\n",
       "      <td>-0.034513</td>\n",
       "      <td>0.037573</td>\n",
       "      <td>0.223898</td>\n",
       "      <td>0.040566</td>\n",
       "      <td>-0.147796</td>\n",
       "      <td>spoof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0009_Neutral_000264.wav</td>\n",
       "      <td>0.172095</td>\n",
       "      <td>-0.198275</td>\n",
       "      <td>0.066159</td>\n",
       "      <td>0.427224</td>\n",
       "      <td>-0.211104</td>\n",
       "      <td>-0.072713</td>\n",
       "      <td>0.121893</td>\n",
       "      <td>-0.114145</td>\n",
       "      <td>-0.076951</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.226404</td>\n",
       "      <td>-0.208485</td>\n",
       "      <td>0.293772</td>\n",
       "      <td>-0.133162</td>\n",
       "      <td>-0.083665</td>\n",
       "      <td>0.085294</td>\n",
       "      <td>0.206235</td>\n",
       "      <td>-0.032916</td>\n",
       "      <td>-0.226173</td>\n",
       "      <td>bonafide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0010_Angry_000683.wav</td>\n",
       "      <td>0.184614</td>\n",
       "      <td>-0.222184</td>\n",
       "      <td>0.001011</td>\n",
       "      <td>0.484833</td>\n",
       "      <td>-0.242463</td>\n",
       "      <td>-0.066548</td>\n",
       "      <td>0.090960</td>\n",
       "      <td>-0.079036</td>\n",
       "      <td>-0.060983</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.248600</td>\n",
       "      <td>-0.226518</td>\n",
       "      <td>0.309246</td>\n",
       "      <td>-0.163002</td>\n",
       "      <td>-0.067097</td>\n",
       "      <td>0.061516</td>\n",
       "      <td>0.215943</td>\n",
       "      <td>-0.000802</td>\n",
       "      <td>-0.142232</td>\n",
       "      <td>bonafide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0009_Surprise_001651.wav</td>\n",
       "      <td>0.193536</td>\n",
       "      <td>-0.199418</td>\n",
       "      <td>0.019337</td>\n",
       "      <td>0.477495</td>\n",
       "      <td>-0.226425</td>\n",
       "      <td>-0.087098</td>\n",
       "      <td>0.083318</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>-0.027949</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.214360</td>\n",
       "      <td>-0.261837</td>\n",
       "      <td>0.307350</td>\n",
       "      <td>-0.191124</td>\n",
       "      <td>-0.070351</td>\n",
       "      <td>0.052526</td>\n",
       "      <td>0.201908</td>\n",
       "      <td>0.021133</td>\n",
       "      <td>-0.130850</td>\n",
       "      <td>bonafide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S2_Surprise2Neutral_0009_001456.wav</td>\n",
       "      <td>0.184930</td>\n",
       "      <td>-0.204854</td>\n",
       "      <td>-0.033367</td>\n",
       "      <td>0.522085</td>\n",
       "      <td>-0.273352</td>\n",
       "      <td>-0.075318</td>\n",
       "      <td>0.064707</td>\n",
       "      <td>0.013866</td>\n",
       "      <td>-0.031558</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.159633</td>\n",
       "      <td>-0.238744</td>\n",
       "      <td>0.253641</td>\n",
       "      <td>-0.203063</td>\n",
       "      <td>-0.029254</td>\n",
       "      <td>0.033224</td>\n",
       "      <td>0.180608</td>\n",
       "      <td>0.033090</td>\n",
       "      <td>-0.120810</td>\n",
       "      <td>spoof</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 770 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              filename         0         1         2  \\\n",
       "0    S2_Surprise2Angry_0010_001420.wav  0.189128 -0.209342 -0.004541   \n",
       "1              0009_Neutral_000264.wav  0.172095 -0.198275  0.066159   \n",
       "2                0010_Angry_000683.wav  0.184614 -0.222184  0.001011   \n",
       "3             0009_Surprise_001651.wav  0.193536 -0.199418  0.019337   \n",
       "4  S2_Surprise2Neutral_0009_001456.wav  0.184930 -0.204854 -0.033367   \n",
       "\n",
       "          3         4         5         6         7         8  ...       759  \\\n",
       "0  0.491101 -0.249494 -0.100917  0.049525  0.006354 -0.017165  ... -0.222937   \n",
       "1  0.427224 -0.211104 -0.072713  0.121893 -0.114145 -0.076951  ... -0.226404   \n",
       "2  0.484833 -0.242463 -0.066548  0.090960 -0.079036 -0.060983  ... -0.248600   \n",
       "3  0.477495 -0.226425 -0.087098  0.083318  0.000034 -0.027949  ... -0.214360   \n",
       "4  0.522085 -0.273352 -0.075318  0.064707  0.013866 -0.031558  ... -0.159633   \n",
       "\n",
       "        760       761       762       763       764       765       766  \\\n",
       "0 -0.228415  0.339616 -0.211259 -0.034513  0.037573  0.223898  0.040566   \n",
       "1 -0.208485  0.293772 -0.133162 -0.083665  0.085294  0.206235 -0.032916   \n",
       "2 -0.226518  0.309246 -0.163002 -0.067097  0.061516  0.215943 -0.000802   \n",
       "3 -0.261837  0.307350 -0.191124 -0.070351  0.052526  0.201908  0.021133   \n",
       "4 -0.238744  0.253641 -0.203063 -0.029254  0.033224  0.180608  0.033090   \n",
       "\n",
       "        767     class  \n",
       "0 -0.147796     spoof  \n",
       "1 -0.226173  bonafide  \n",
       "2 -0.142232  bonafide  \n",
       "3 -0.130850  bonafide  \n",
       "4 -0.120810     spoof  \n",
       "\n",
       "[5 rows x 770 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1936a2a6-db77-402d-a266-67951d77b5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0008_Happy_000882.wav</td>\n",
       "      <td>0.160846</td>\n",
       "      <td>-0.202603</td>\n",
       "      <td>-0.029431</td>\n",
       "      <td>0.452209</td>\n",
       "      <td>-0.256997</td>\n",
       "      <td>-0.039944</td>\n",
       "      <td>0.024706</td>\n",
       "      <td>0.023729</td>\n",
       "      <td>-0.011878</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.180685</td>\n",
       "      <td>-0.249422</td>\n",
       "      <td>0.183773</td>\n",
       "      <td>-0.200146</td>\n",
       "      <td>-0.057717</td>\n",
       "      <td>0.020661</td>\n",
       "      <td>0.199693</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>-0.093136</td>\n",
       "      <td>bonafide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S7_Sad2Surprise_0008_001054.wav</td>\n",
       "      <td>0.158242</td>\n",
       "      <td>-0.217189</td>\n",
       "      <td>-0.025827</td>\n",
       "      <td>0.468232</td>\n",
       "      <td>-0.225677</td>\n",
       "      <td>-0.085270</td>\n",
       "      <td>0.022428</td>\n",
       "      <td>0.055285</td>\n",
       "      <td>0.042900</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196830</td>\n",
       "      <td>-0.277372</td>\n",
       "      <td>0.293661</td>\n",
       "      <td>-0.198013</td>\n",
       "      <td>-0.036369</td>\n",
       "      <td>0.030010</td>\n",
       "      <td>0.206655</td>\n",
       "      <td>0.038444</td>\n",
       "      <td>-0.128283</td>\n",
       "      <td>spoof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S5_Sad2Angry_0008_001358.wav</td>\n",
       "      <td>0.300717</td>\n",
       "      <td>-0.307127</td>\n",
       "      <td>0.067005</td>\n",
       "      <td>0.634999</td>\n",
       "      <td>-0.274562</td>\n",
       "      <td>-0.142308</td>\n",
       "      <td>0.084050</td>\n",
       "      <td>-0.094868</td>\n",
       "      <td>0.012020</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.342937</td>\n",
       "      <td>-0.229861</td>\n",
       "      <td>0.933153</td>\n",
       "      <td>-0.219925</td>\n",
       "      <td>-0.061918</td>\n",
       "      <td>0.098574</td>\n",
       "      <td>0.346627</td>\n",
       "      <td>0.086339</td>\n",
       "      <td>-0.178897</td>\n",
       "      <td>spoof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S7_Sad2Angry_0008_001324.wav</td>\n",
       "      <td>0.162972</td>\n",
       "      <td>-0.258586</td>\n",
       "      <td>0.018852</td>\n",
       "      <td>0.473129</td>\n",
       "      <td>-0.274531</td>\n",
       "      <td>-0.085156</td>\n",
       "      <td>0.070843</td>\n",
       "      <td>-0.017749</td>\n",
       "      <td>-0.029502</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.272154</td>\n",
       "      <td>-0.232923</td>\n",
       "      <td>0.415868</td>\n",
       "      <td>-0.192512</td>\n",
       "      <td>-0.059498</td>\n",
       "      <td>0.054649</td>\n",
       "      <td>0.270987</td>\n",
       "      <td>0.019338</td>\n",
       "      <td>-0.149768</td>\n",
       "      <td>spoof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S5_Sad2Happy_0008_001242.wav</td>\n",
       "      <td>0.207343</td>\n",
       "      <td>-0.298982</td>\n",
       "      <td>0.052051</td>\n",
       "      <td>0.449070</td>\n",
       "      <td>-0.257353</td>\n",
       "      <td>-0.134474</td>\n",
       "      <td>0.098110</td>\n",
       "      <td>-0.093671</td>\n",
       "      <td>-0.066941</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.328760</td>\n",
       "      <td>-0.170260</td>\n",
       "      <td>0.545209</td>\n",
       "      <td>-0.195658</td>\n",
       "      <td>-0.058123</td>\n",
       "      <td>0.077800</td>\n",
       "      <td>0.319440</td>\n",
       "      <td>0.032278</td>\n",
       "      <td>-0.089926</td>\n",
       "      <td>spoof</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 770 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          filename         0         1         2         3  \\\n",
       "0            0008_Happy_000882.wav  0.160846 -0.202603 -0.029431  0.452209   \n",
       "1  S7_Sad2Surprise_0008_001054.wav  0.158242 -0.217189 -0.025827  0.468232   \n",
       "2     S5_Sad2Angry_0008_001358.wav  0.300717 -0.307127  0.067005  0.634999   \n",
       "3     S7_Sad2Angry_0008_001324.wav  0.162972 -0.258586  0.018852  0.473129   \n",
       "4     S5_Sad2Happy_0008_001242.wav  0.207343 -0.298982  0.052051  0.449070   \n",
       "\n",
       "          4         5         6         7         8  ...       759       760  \\\n",
       "0 -0.256997 -0.039944  0.024706  0.023729 -0.011878  ... -0.180685 -0.249422   \n",
       "1 -0.225677 -0.085270  0.022428  0.055285  0.042900  ... -0.196830 -0.277372   \n",
       "2 -0.274562 -0.142308  0.084050 -0.094868  0.012020  ... -0.342937 -0.229861   \n",
       "3 -0.274531 -0.085156  0.070843 -0.017749 -0.029502  ... -0.272154 -0.232923   \n",
       "4 -0.257353 -0.134474  0.098110 -0.093671 -0.066941  ... -0.328760 -0.170260   \n",
       "\n",
       "        761       762       763       764       765       766       767  \\\n",
       "0  0.183773 -0.200146 -0.057717  0.020661  0.199693  0.009091 -0.093136   \n",
       "1  0.293661 -0.198013 -0.036369  0.030010  0.206655  0.038444 -0.128283   \n",
       "2  0.933153 -0.219925 -0.061918  0.098574  0.346627  0.086339 -0.178897   \n",
       "3  0.415868 -0.192512 -0.059498  0.054649  0.270987  0.019338 -0.149768   \n",
       "4  0.545209 -0.195658 -0.058123  0.077800  0.319440  0.032278 -0.089926   \n",
       "\n",
       "      class  \n",
       "0  bonafide  \n",
       "1     spoof  \n",
       "2     spoof  \n",
       "3     spoof  \n",
       "4     spoof  \n",
       "\n",
       "[5 rows x 770 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3f04456-1df1-444c-a737-a9cc146ef769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S1_Angry2Happy_0004_000578.wav</td>\n",
       "      <td>0.192144</td>\n",
       "      <td>-0.230899</td>\n",
       "      <td>-0.007889</td>\n",
       "      <td>0.463364</td>\n",
       "      <td>-0.309516</td>\n",
       "      <td>-0.119908</td>\n",
       "      <td>0.047797</td>\n",
       "      <td>0.007947</td>\n",
       "      <td>-0.044678</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.248091</td>\n",
       "      <td>-0.187253</td>\n",
       "      <td>0.327632</td>\n",
       "      <td>-0.248353</td>\n",
       "      <td>-0.045072</td>\n",
       "      <td>0.034011</td>\n",
       "      <td>0.238101</td>\n",
       "      <td>0.035052</td>\n",
       "      <td>-0.160326</td>\n",
       "      <td>spoof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S2_Happy2Angry_0005_001020.wav</td>\n",
       "      <td>0.164120</td>\n",
       "      <td>-0.207680</td>\n",
       "      <td>-0.018232</td>\n",
       "      <td>0.457945</td>\n",
       "      <td>-0.255097</td>\n",
       "      <td>-0.084890</td>\n",
       "      <td>0.056021</td>\n",
       "      <td>0.022304</td>\n",
       "      <td>-0.050971</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.187700</td>\n",
       "      <td>-0.247315</td>\n",
       "      <td>0.268146</td>\n",
       "      <td>-0.217452</td>\n",
       "      <td>-0.067052</td>\n",
       "      <td>0.028347</td>\n",
       "      <td>0.188930</td>\n",
       "      <td>0.023942</td>\n",
       "      <td>-0.205236</td>\n",
       "      <td>spoof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S1_Neutral2Surprise_0001_000215.wav</td>\n",
       "      <td>0.185979</td>\n",
       "      <td>-0.267814</td>\n",
       "      <td>0.030250</td>\n",
       "      <td>0.406555</td>\n",
       "      <td>-0.268612</td>\n",
       "      <td>-0.113836</td>\n",
       "      <td>0.106833</td>\n",
       "      <td>-0.081320</td>\n",
       "      <td>-0.076577</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.286851</td>\n",
       "      <td>-0.213667</td>\n",
       "      <td>0.461657</td>\n",
       "      <td>-0.176494</td>\n",
       "      <td>-0.071047</td>\n",
       "      <td>0.060256</td>\n",
       "      <td>0.274009</td>\n",
       "      <td>0.011276</td>\n",
       "      <td>-0.211352</td>\n",
       "      <td>spoof</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0006_Happy_000827.wav</td>\n",
       "      <td>0.193432</td>\n",
       "      <td>-0.242063</td>\n",
       "      <td>0.019229</td>\n",
       "      <td>0.464806</td>\n",
       "      <td>-0.215070</td>\n",
       "      <td>-0.085839</td>\n",
       "      <td>0.088731</td>\n",
       "      <td>-0.027413</td>\n",
       "      <td>-0.025742</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.281962</td>\n",
       "      <td>-0.227900</td>\n",
       "      <td>0.332402</td>\n",
       "      <td>-0.189493</td>\n",
       "      <td>-0.079213</td>\n",
       "      <td>0.063780</td>\n",
       "      <td>0.249012</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>-0.127866</td>\n",
       "      <td>bonafide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S2_Neutral2Surprise_0001_000313.wav</td>\n",
       "      <td>0.156264</td>\n",
       "      <td>-0.239932</td>\n",
       "      <td>0.005776</td>\n",
       "      <td>0.503117</td>\n",
       "      <td>-0.245035</td>\n",
       "      <td>-0.068548</td>\n",
       "      <td>0.067865</td>\n",
       "      <td>-0.045244</td>\n",
       "      <td>-0.044124</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.218631</td>\n",
       "      <td>-0.249604</td>\n",
       "      <td>0.376530</td>\n",
       "      <td>-0.202699</td>\n",
       "      <td>-0.070528</td>\n",
       "      <td>0.045023</td>\n",
       "      <td>0.216239</td>\n",
       "      <td>-0.004558</td>\n",
       "      <td>-0.150248</td>\n",
       "      <td>spoof</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 770 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              filename         0         1         2  \\\n",
       "0       S1_Angry2Happy_0004_000578.wav  0.192144 -0.230899 -0.007889   \n",
       "1       S2_Happy2Angry_0005_001020.wav  0.164120 -0.207680 -0.018232   \n",
       "2  S1_Neutral2Surprise_0001_000215.wav  0.185979 -0.267814  0.030250   \n",
       "3                0006_Happy_000827.wav  0.193432 -0.242063  0.019229   \n",
       "4  S2_Neutral2Surprise_0001_000313.wav  0.156264 -0.239932  0.005776   \n",
       "\n",
       "          3         4         5         6         7         8  ...       759  \\\n",
       "0  0.463364 -0.309516 -0.119908  0.047797  0.007947 -0.044678  ... -0.248091   \n",
       "1  0.457945 -0.255097 -0.084890  0.056021  0.022304 -0.050971  ... -0.187700   \n",
       "2  0.406555 -0.268612 -0.113836  0.106833 -0.081320 -0.076577  ... -0.286851   \n",
       "3  0.464806 -0.215070 -0.085839  0.088731 -0.027413 -0.025742  ... -0.281962   \n",
       "4  0.503117 -0.245035 -0.068548  0.067865 -0.045244 -0.044124  ... -0.218631   \n",
       "\n",
       "        760       761       762       763       764       765       766  \\\n",
       "0 -0.187253  0.327632 -0.248353 -0.045072  0.034011  0.238101  0.035052   \n",
       "1 -0.247315  0.268146 -0.217452 -0.067052  0.028347  0.188930  0.023942   \n",
       "2 -0.213667  0.461657 -0.176494 -0.071047  0.060256  0.274009  0.011276   \n",
       "3 -0.227900  0.332402 -0.189493 -0.079213  0.063780  0.249012  0.013761   \n",
       "4 -0.249604  0.376530 -0.202699 -0.070528  0.045023  0.216239 -0.004558   \n",
       "\n",
       "        767     class  \n",
       "0 -0.160326     spoof  \n",
       "1 -0.205236     spoof  \n",
       "2 -0.211352     spoof  \n",
       "3 -0.127866  bonafide  \n",
       "4 -0.150248     spoof  \n",
       "\n",
       "[5 rows x 770 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe37619-36ce-41c5-a2f3-f1c9b75e755f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# C WAVLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3ec2482-e7a0-4f22-b02e-6c4404216abe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "854/854 [==============================] - 13s 14ms/step - loss: 0.3111 - accuracy: 0.8689 - val_loss: 0.3008 - val_accuracy: 0.8699 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.2418 - accuracy: 0.8993 - val_loss: 0.3834 - val_accuracy: 0.8442 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.2219 - accuracy: 0.9100 - val_loss: 0.4657 - val_accuracy: 0.8023 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.2084 - accuracy: 0.9153 - val_loss: 0.4966 - val_accuracy: 0.7902 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.1923 - accuracy: 0.9208 - val_loss: 0.2059 - val_accuracy: 0.9133 - lr: 5.0000e-04\n",
      "Epoch 6/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.1870 - accuracy: 0.9244 - val_loss: 0.1974 - val_accuracy: 0.9195 - lr: 5.0000e-04\n",
      "Epoch 7/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.1798 - accuracy: 0.9278 - val_loss: 0.2814 - val_accuracy: 0.8856 - lr: 5.0000e-04\n",
      "Epoch 8/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.1796 - accuracy: 0.9302 - val_loss: 0.2781 - val_accuracy: 0.8879 - lr: 5.0000e-04\n",
      "Epoch 9/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.1806 - accuracy: 0.9289 - val_loss: 0.2149 - val_accuracy: 0.9109 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.1658 - accuracy: 0.9360 - val_loss: 0.1925 - val_accuracy: 0.9225 - lr: 2.5000e-04\n",
      "Epoch 11/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.1644 - accuracy: 0.9374 - val_loss: 0.2469 - val_accuracy: 0.9023 - lr: 2.5000e-04\n",
      "Epoch 12/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.1645 - accuracy: 0.9358 - val_loss: 0.2137 - val_accuracy: 0.9132 - lr: 2.5000e-04\n",
      "Epoch 13/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.1591 - accuracy: 0.9382 - val_loss: 0.1859 - val_accuracy: 0.9229 - lr: 2.5000e-04\n",
      "Epoch 14/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.1565 - accuracy: 0.9410 - val_loss: 0.1877 - val_accuracy: 0.9227 - lr: 2.5000e-04\n",
      "Epoch 15/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.1564 - accuracy: 0.9421 - val_loss: 0.1826 - val_accuracy: 0.9260 - lr: 2.5000e-04\n",
      "Epoch 16/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.1594 - accuracy: 0.9389 - val_loss: 0.2990 - val_accuracy: 0.8784 - lr: 2.5000e-04\n",
      "Epoch 17/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.1541 - accuracy: 0.9410 - val_loss: 0.2355 - val_accuracy: 0.9076 - lr: 2.5000e-04\n",
      "Epoch 18/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.1552 - accuracy: 0.9399 - val_loss: 0.1827 - val_accuracy: 0.9240 - lr: 2.5000e-04\n",
      "Epoch 19/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.1552 - accuracy: 0.9409 - val_loss: 0.1904 - val_accuracy: 0.9221 - lr: 1.2500e-04\n",
      "Epoch 20/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.1541 - accuracy: 0.9417 - val_loss: 0.1818 - val_accuracy: 0.9271 - lr: 1.2500e-04\n",
      "Epoch 21/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.1499 - accuracy: 0.9436 - val_loss: 0.2021 - val_accuracy: 0.9192 - lr: 1.2500e-04\n",
      "Epoch 22/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.1454 - accuracy: 0.9461 - val_loss: 0.2362 - val_accuracy: 0.9074 - lr: 1.2500e-04\n",
      "Epoch 23/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.1458 - accuracy: 0.9441 - val_loss: 0.1759 - val_accuracy: 0.9291 - lr: 1.2500e-04\n",
      "Epoch 24/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.1443 - accuracy: 0.9458 - val_loss: 0.1881 - val_accuracy: 0.9263 - lr: 1.2500e-04\n",
      "Epoch 25/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.1514 - accuracy: 0.9427 - val_loss: 0.1913 - val_accuracy: 0.9221 - lr: 1.2500e-04\n",
      "Epoch 26/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.1468 - accuracy: 0.9452 - val_loss: 0.1783 - val_accuracy: 0.9295 - lr: 1.2500e-04\n",
      "Epoch 27/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.1469 - accuracy: 0.9453 - val_loss: 0.2067 - val_accuracy: 0.9203 - lr: 6.2500e-05\n",
      "Epoch 28/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.1466 - accuracy: 0.9450 - val_loss: 0.1798 - val_accuracy: 0.9280 - lr: 6.2500e-05\n",
      "547/547 [==============================] - 1s 2ms/step\n",
      "547/547 [==============================] - 1s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    bonafide     0.6073    0.9629    0.7448      3500\n",
      "       spoof     0.9891    0.8444    0.9110     14000\n",
      "\n",
      "    accuracy                         0.8681     17500\n",
      "   macro avg     0.7982    0.9036    0.8279     17500\n",
      "weighted avg     0.9128    0.8681    0.8778     17500\n",
      "\n",
      "Equal Error Rate (EER): 9.0821%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, roc_curve\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.metrics import roc_curve \n",
    "import sklearn.metrics  \n",
    "\n",
    "def load_and_preprocess_data(train_file, dev_file, test_file):\n",
    "    train = pd.read_csv(train_file)\n",
    "    dev = pd.read_csv(dev_file)\n",
    "    test = pd.read_csv(test_file)\n",
    "    \n",
    "    X_train = train.iloc[:, 1:-1].values\n",
    "    y_train = train['class'].values\n",
    "\n",
    "    X_dev = dev.iloc[:, 1:-1].values\n",
    "    y_dev = dev['class'].values\n",
    "\n",
    "    X_test = test.iloc[:, 1:-1].values\n",
    "    y_test = test['class'].values\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "    y_dev = encoder.transform(y_dev)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_dev = np.expand_dims(X_dev, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "    \n",
    "    return X_train, y_train, X_dev, y_dev, X_test, y_test\n",
    "\n",
    "def compute_eer(label, pred, positive_label=1):\n",
    "    fpr, tpr, threshold = sklearn.metrics.roc_curve(label, pred, pos_label=positive_label)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer = (eer_1 + eer_2) / 2\n",
    "    return eer\n",
    "    \n",
    "def build_cnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(), \n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),   \n",
    "        Conv1D(128, kernel_size=3, activation='relu'),\n",
    "        BatchNormalization(),  \n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),  \n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(2, activation='softmax') \n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_cnn(train_file, dev_file, test_file, epochs=10, batch_size=32):\n",
    "    \n",
    "    X_train, y_train, X_dev, y_dev, X_test, y_test = load_and_preprocess_data(train_file, dev_file, test_file)\n",
    "    \n",
    "    model = build_cnn_model(input_shape=(X_train.shape[1], 1))\n",
    "  \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=epochs, batch_size=batch_size,\n",
    "              callbacks=[reduce_lr, early_stop])\n",
    "    \n",
    "    y_pred_prob = model.predict(X_test)[:, 1]  \n",
    "    \n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    print(classification_report(y_test, y_pred, target_names=['bonafide', 'spoof'], digits=4))\n",
    "\n",
    "    eer = compute_eer(y_test, y_pred_prob, positive_label=1)\n",
    "    eer_percentage = eer * 100  \n",
    "    print(f'Equal Error Rate (EER): {eer_percentage:.4f}%')\n",
    "    \n",
    "    return model\n",
    "\n",
    "train_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/WavLM-EMOFAKE/Chinese_train_WavLM.csv'\n",
    "dev_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/WavLM-EMOFAKE/Chinese_dev_WavLM.csv'\n",
    "test_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/WavLM-EMOFAKE/Chinese_test_WavLM.csv'\n",
    "\n",
    "cnn_model = train_and_evaluate_cnn(train_file, dev_file, test_file, epochs=100, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73e96f5-2be7-4336-85c6-506abf5d02b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# E wavlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad7a77cb-e4d2-4964-a58e-60ed783bc600",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-24 21:07:02.708872: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-24 21:07:03.055179: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-24 21:07:03.055256: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-24 21:07:03.112119: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-24 21:07:03.239138: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-24 21:07:04.497651: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-09-24 21:07:10.416842: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-24 21:07:10.571177: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-24 21:07:10.571267: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-24 21:07:10.573748: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-24 21:07:10.573916: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-24 21:07:10.574020: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-24 21:07:10.730830: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-24 21:07:10.730899: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-24 21:07:10.730908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-09-24 21:07:10.730948: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-24 21:07:10.730965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3620 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 6GB Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-24 21:07:12.188740: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-09-24 21:07:12.433823: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-09-24 21:07:12.800873: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-09-24 21:07:13.651373: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f6e71dea0f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-09-24 21:07:13.651425: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 6GB Laptop GPU, Compute Capability 8.6\n",
      "2024-09-24 21:07:13.666868: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727192233.784415    4447 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "854/854 [==============================] - 20s 19ms/step - loss: 0.3721 - accuracy: 0.8367 - val_loss: 0.1609 - val_accuracy: 0.9423 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "854/854 [==============================] - 14s 17ms/step - loss: 0.2796 - accuracy: 0.8829 - val_loss: 0.2133 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "854/854 [==============================] - 14s 17ms/step - loss: 0.2544 - accuracy: 0.8970 - val_loss: 0.1409 - val_accuracy: 0.9443 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "854/854 [==============================] - 15s 17ms/step - loss: 0.2329 - accuracy: 0.9053 - val_loss: 0.0919 - val_accuracy: 0.9679 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "854/854 [==============================] - 13s 16ms/step - loss: 0.2311 - accuracy: 0.9045 - val_loss: 0.1189 - val_accuracy: 0.9541 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "854/854 [==============================] - 13s 16ms/step - loss: 0.2126 - accuracy: 0.9132 - val_loss: 0.1779 - val_accuracy: 0.9234 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "854/854 [==============================] - 13s 15ms/step - loss: 0.2106 - accuracy: 0.9162 - val_loss: 0.0704 - val_accuracy: 0.9786 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "854/854 [==============================] - 13s 15ms/step - loss: 0.2013 - accuracy: 0.9222 - val_loss: 0.1434 - val_accuracy: 0.9399 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "854/854 [==============================] - 13s 16ms/step - loss: 0.1965 - accuracy: 0.9216 - val_loss: 0.0810 - val_accuracy: 0.9720 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "854/854 [==============================] - 14s 17ms/step - loss: 0.1970 - accuracy: 0.9222 - val_loss: 0.1004 - val_accuracy: 0.9623 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "854/854 [==============================] - 14s 16ms/step - loss: 0.1826 - accuracy: 0.9275 - val_loss: 0.0946 - val_accuracy: 0.9647 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "854/854 [==============================] - 14s 17ms/step - loss: 0.1761 - accuracy: 0.9313 - val_loss: 0.1410 - val_accuracy: 0.9400 - lr: 5.0000e-04\n",
      "547/547 [==============================] - 2s 4ms/step\n",
      "547/547 [==============================] - 2s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    bonafide     0.5630    0.9777    0.7146      3500\n",
      "       spoof     0.9932    0.8103    0.8925     14000\n",
      "\n",
      "    accuracy                         0.8438     17500\n",
      "   macro avg     0.7781    0.8940    0.8035     17500\n",
      "weighted avg     0.9071    0.8438    0.8569     17500\n",
      "\n",
      "Equal Error Rate (EER): 7.4286%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, roc_curve\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.metrics import roc_curve \n",
    "import sklearn.metrics  \n",
    "\n",
    "def load_and_preprocess_data(train_file, dev_file, test_file):\n",
    "    train = pd.read_csv(train_file)\n",
    "    dev = pd.read_csv(dev_file)\n",
    "    test = pd.read_csv(test_file)\n",
    "    \n",
    "    X_train = train.iloc[:, 1:-1].values\n",
    "    y_train = train['class'].values\n",
    "\n",
    "    X_dev = dev.iloc[:, 1:-1].values\n",
    "    y_dev = dev['class'].values\n",
    "\n",
    "    X_test = test.iloc[:, 1:-1].values\n",
    "    y_test = test['class'].values\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "    y_dev = encoder.transform(y_dev)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_dev = np.expand_dims(X_dev, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "    \n",
    "    return X_train, y_train, X_dev, y_dev, X_test, y_test\n",
    "\n",
    "def compute_eer(label, pred, positive_label=1):\n",
    "    fpr, tpr, threshold = sklearn.metrics.roc_curve(label, pred, pos_label=positive_label)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer = (eer_1 + eer_2) / 2\n",
    "    return eer\n",
    "    \n",
    "def build_cnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(), \n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),   \n",
    "        Conv1D(128, kernel_size=3, activation='relu'),\n",
    "        BatchNormalization(),  \n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),  \n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(2, activation='softmax') \n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_cnn(train_file, dev_file, test_file, epochs=10, batch_size=32):\n",
    "    \n",
    "    X_train, y_train, X_dev, y_dev, X_test, y_test = load_and_preprocess_data(train_file, dev_file, test_file)\n",
    "    \n",
    "    model = build_cnn_model(input_shape=(X_train.shape[1], 1))\n",
    "  \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=epochs, batch_size=batch_size,\n",
    "              callbacks=[reduce_lr, early_stop])\n",
    "    \n",
    "    y_pred_prob = model.predict(X_test)[:, 1]  \n",
    "    \n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    print(classification_report(y_test, y_pred, target_names=['bonafide', 'spoof'], digits=4))\n",
    "\n",
    "    eer = compute_eer(y_test, y_pred_prob, positive_label=1)\n",
    "    eer_percentage = eer * 100  \n",
    "    print(f'Equal Error Rate (EER): {eer_percentage:.4f}%')\n",
    "    \n",
    "    return model\n",
    "\n",
    "train_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/WavLM-EMOFAKE/English_train_WavLM.csv'\n",
    "dev_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/WavLM-EMOFAKE/English_dev_WavLM.csv'\n",
    "test_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/WavLM-EMOFAKE/English_test_WavLM.csv'\n",
    "\n",
    "cnn_model = train_and_evaluate_cnn(train_file, dev_file, test_file, epochs=100, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8674385-7334-470c-9dd3-a127cc1ad124",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# C XVECTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e00043a1-7cd2-493d-9fe5-9fa95cd8cb9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "854/854 [==============================] - 13s 12ms/step - loss: 0.0184 - accuracy: 0.9943 - val_loss: 0.0047 - val_accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "854/854 [==============================] - 9s 11ms/step - loss: 0.0075 - accuracy: 0.9979 - val_loss: 0.0744 - val_accuracy: 0.9746 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "854/854 [==============================] - 9s 11ms/step - loss: 0.0068 - accuracy: 0.9981 - val_loss: 0.0551 - val_accuracy: 0.9791 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "854/854 [==============================] - 9s 11ms/step - loss: 0.0046 - accuracy: 0.9987 - val_loss: 0.0046 - val_accuracy: 0.9986 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "854/854 [==============================] - 9s 11ms/step - loss: 0.0068 - accuracy: 0.9976 - val_loss: 0.0019 - val_accuracy: 0.9996 - lr: 5.0000e-04\n",
      "Epoch 6/100\n",
      "854/854 [==============================] - 9s 11ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.0189 - val_accuracy: 0.9929 - lr: 5.0000e-04\n",
      "Epoch 7/100\n",
      "854/854 [==============================] - 9s 11ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 0.0028 - val_accuracy: 0.9991 - lr: 5.0000e-04\n",
      "Epoch 8/100\n",
      "854/854 [==============================] - 9s 11ms/step - loss: 0.0049 - accuracy: 0.9987 - val_loss: 0.0024 - val_accuracy: 0.9993 - lr: 5.0000e-04\n",
      "Epoch 9/100\n",
      "854/854 [==============================] - 9s 11ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.0019 - val_accuracy: 0.9995 - lr: 2.5000e-04\n",
      "Epoch 10/100\n",
      "854/854 [==============================] - 9s 11ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0017 - val_accuracy: 0.9993 - lr: 2.5000e-04\n",
      "Epoch 11/100\n",
      "854/854 [==============================] - 9s 11ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0012 - val_accuracy: 0.9997 - lr: 2.5000e-04\n",
      "Epoch 12/100\n",
      "854/854 [==============================] - 10s 11ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.0020 - val_accuracy: 0.9991 - lr: 2.5000e-04\n",
      "Epoch 13/100\n",
      "854/854 [==============================] - 9s 11ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 0.0042 - val_accuracy: 0.9985 - lr: 2.5000e-04\n",
      "Epoch 14/100\n",
      "854/854 [==============================] - 10s 11ms/step - loss: 0.0013 - accuracy: 0.9995 - val_loss: 0.0031 - val_accuracy: 0.9988 - lr: 2.5000e-04\n",
      "Epoch 15/100\n",
      "854/854 [==============================] - 9s 11ms/step - loss: 8.6937e-04 - accuracy: 0.9997 - val_loss: 0.0026 - val_accuracy: 0.9988 - lr: 1.2500e-04\n",
      "Epoch 16/100\n",
      "854/854 [==============================] - 9s 11ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.0042 - val_accuracy: 0.9985 - lr: 1.2500e-04\n",
      "547/547 [==============================] - 1s 2ms/step\n",
      "547/547 [==============================] - 1s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    bonafide     0.4600    0.9977    0.6296      3500\n",
      "       spoof     0.9992    0.7071    0.8282     14000\n",
      "\n",
      "    accuracy                         0.7653     17500\n",
      "   macro avg     0.7296    0.8524    0.7289     17500\n",
      "weighted avg     0.8913    0.7653    0.7885     17500\n",
      "\n",
      "Equal Error Rate (EER): 5.6821%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, roc_curve\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.metrics import roc_curve \n",
    "import sklearn.metrics  \n",
    "\n",
    "def load_and_preprocess_data(train_file, dev_file, test_file):\n",
    "    train = pd.read_csv(train_file)\n",
    "    dev = pd.read_csv(dev_file)\n",
    "    test = pd.read_csv(test_file)\n",
    "    \n",
    "    X_train = train.iloc[:, 1:-1].values\n",
    "    y_train = train['class'].values\n",
    "\n",
    "    X_dev = dev.iloc[:, 1:-1].values\n",
    "    y_dev = dev['class'].values\n",
    "\n",
    "    X_test = test.iloc[:, 1:-1].values\n",
    "    y_test = test['class'].values\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "    y_dev = encoder.transform(y_dev)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_dev = np.expand_dims(X_dev, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "    \n",
    "    return X_train, y_train, X_dev, y_dev, X_test, y_test\n",
    "\n",
    "def compute_eer(label, pred, positive_label=1):\n",
    "    fpr, tpr, threshold = sklearn.metrics.roc_curve(label, pred, pos_label=positive_label)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer = (eer_1 + eer_2) / 2\n",
    "    return eer\n",
    "    \n",
    "def build_cnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(), \n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),   \n",
    "        Conv1D(128, kernel_size=3, activation='relu'),\n",
    "        BatchNormalization(),  \n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),  \n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(2, activation='softmax') \n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_cnn(train_file, dev_file, test_file, epochs=10, batch_size=32):\n",
    "    \n",
    "    X_train, y_train, X_dev, y_dev, X_test, y_test = load_and_preprocess_data(train_file, dev_file, test_file)\n",
    "    \n",
    "    model = build_cnn_model(input_shape=(X_train.shape[1], 1))\n",
    "  \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=epochs, batch_size=batch_size,\n",
    "              callbacks=[reduce_lr, early_stop])\n",
    "    \n",
    "    y_pred_prob = model.predict(X_test)[:, 1]  \n",
    "    \n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    print(classification_report(y_test, y_pred, target_names=['bonafide', 'spoof'], digits=4))\n",
    "\n",
    "    eer = compute_eer(y_test, y_pred_prob, positive_label=1)\n",
    "    eer_percentage = eer * 100  \n",
    "    print(f'Equal Error Rate (EER): {eer_percentage:.4f}%')\n",
    "    \n",
    "    return model\n",
    "# Example usage:\n",
    "train_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/xvector-EMOFAKE/Chinese_train_xvector.csv'\n",
    "dev_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/xvector-EMOFAKE/Chinese_dev_xvector.csv'\n",
    "test_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/xvector-EMOFAKE/Chinese_test_xvector.csv'\n",
    "\n",
    "# Call the function to train and evaluate the CNN model\n",
    "cnn_model = train_and_evaluate_cnn(train_file, dev_file, test_file, epochs=100, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76de8f92-e024-4cec-94d0-a99a35626d45",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# E Xvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9a795b4-610f-4609-8d41-fa04a23ef0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "854/854 [==============================] - 11s 12ms/step - loss: 0.0181 - accuracy: 0.9934 - val_loss: 0.0041 - val_accuracy: 0.9984 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "854/854 [==============================] - 10s 11ms/step - loss: 0.0094 - accuracy: 0.9968 - val_loss: 0.0069 - val_accuracy: 0.9975 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "854/854 [==============================] - 9s 11ms/step - loss: 0.0100 - accuracy: 0.9971 - val_loss: 0.0212 - val_accuracy: 0.9934 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "854/854 [==============================] - 10s 11ms/step - loss: 0.0084 - accuracy: 0.9974 - val_loss: 0.0029 - val_accuracy: 0.9992 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "854/854 [==============================] - 9s 11ms/step - loss: 0.0057 - accuracy: 0.9980 - val_loss: 0.0281 - val_accuracy: 0.9902 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "854/854 [==============================] - 9s 11ms/step - loss: 0.0077 - accuracy: 0.9978 - val_loss: 0.0040 - val_accuracy: 0.9986 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "854/854 [==============================] - 9s 11ms/step - loss: 0.0058 - accuracy: 0.9985 - val_loss: 0.0081 - val_accuracy: 0.9978 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "854/854 [==============================] - 9s 10ms/step - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.0046 - val_accuracy: 0.9984 - lr: 5.0000e-04\n",
      "Epoch 9/100\n",
      "854/854 [==============================] - 9s 11ms/step - loss: 0.0036 - accuracy: 0.9988 - val_loss: 0.0078 - val_accuracy: 0.9975 - lr: 5.0000e-04\n",
      "547/547 [==============================] - 1s 2ms/step\n",
      "547/547 [==============================] - 1s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    bonafide     0.3080    0.9997    0.4709      3500\n",
      "       spoof     0.9998    0.4385    0.6096     14000\n",
      "\n",
      "    accuracy                         0.5507     17500\n",
      "   macro avg     0.6539    0.7191    0.5403     17500\n",
      "weighted avg     0.8615    0.5507    0.5819     17500\n",
      "\n",
      "Equal Error Rate (EER): 5.7964%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, roc_curve\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.metrics import roc_curve \n",
    "import sklearn.metrics  \n",
    "\n",
    "def load_and_preprocess_data(train_file, dev_file, test_file):\n",
    "    train = pd.read_csv(train_file)\n",
    "    dev = pd.read_csv(dev_file)\n",
    "    test = pd.read_csv(test_file)\n",
    "    \n",
    "    X_train = train.iloc[:, 1:-1].values\n",
    "    y_train = train['class'].values\n",
    "\n",
    "    X_dev = dev.iloc[:, 1:-1].values\n",
    "    y_dev = dev['class'].values\n",
    "\n",
    "    X_test = test.iloc[:, 1:-1].values\n",
    "    y_test = test['class'].values\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "    y_dev = encoder.transform(y_dev)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_dev = np.expand_dims(X_dev, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "    \n",
    "    return X_train, y_train, X_dev, y_dev, X_test, y_test\n",
    "\n",
    "def compute_eer(label, pred, positive_label=1):\n",
    "    fpr, tpr, threshold = sklearn.metrics.roc_curve(label, pred, pos_label=positive_label)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer = (eer_1 + eer_2) / 2\n",
    "    return eer\n",
    "    \n",
    "def build_cnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(), \n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),   \n",
    "        Conv1D(128, kernel_size=3, activation='relu'),\n",
    "        BatchNormalization(),  \n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),  \n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(2, activation='softmax') \n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_cnn(train_file, dev_file, test_file, epochs=10, batch_size=32):\n",
    "    \n",
    "    X_train, y_train, X_dev, y_dev, X_test, y_test = load_and_preprocess_data(train_file, dev_file, test_file)\n",
    "    \n",
    "    model = build_cnn_model(input_shape=(X_train.shape[1], 1))\n",
    "  \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=epochs, batch_size=batch_size,\n",
    "              callbacks=[reduce_lr, early_stop])\n",
    "    \n",
    "    y_pred_prob = model.predict(X_test)[:, 1]  \n",
    "    \n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    print(classification_report(y_test, y_pred, target_names=['bonafide', 'spoof'], digits=4))\n",
    "\n",
    "    eer = compute_eer(y_test, y_pred_prob, positive_label=1)\n",
    "    eer_percentage = eer * 100  \n",
    "    print(f'Equal Error Rate (EER): {eer_percentage:.4f}%')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "train_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/xvector-EMOFAKE/English_train_xvector.csv'\n",
    "dev_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/xvector-EMOFAKE/English_dev_xvector.csv'\n",
    "test_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/xvector-EMOFAKE/English_test_xvector.csv'\n",
    "\n",
    "# Call the function to train and evaluate the CNN model\n",
    "cnn_model = train_and_evaluate_cnn(train_file, dev_file, test_file, epochs=100, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b53c907-673a-4e76-b4d9-c19b0ca7985b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# E MERT-v1-330M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c65c879b-caf3-4b7b-9b88-ea4725c6d9b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "854/854 [==============================] - 16s 16ms/step - loss: 0.2788 - accuracy: 0.8816 - val_loss: 0.1461 - val_accuracy: 0.9392 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "854/854 [==============================] - 13s 16ms/step - loss: 0.1003 - accuracy: 0.9611 - val_loss: 0.1198 - val_accuracy: 0.9513 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "854/854 [==============================] - 13s 16ms/step - loss: 0.0809 - accuracy: 0.9690 - val_loss: 0.0613 - val_accuracy: 0.9790 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "854/854 [==============================] - 13s 15ms/step - loss: 0.0628 - accuracy: 0.9768 - val_loss: 0.0737 - val_accuracy: 0.9724 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "854/854 [==============================] - 13s 15ms/step - loss: 0.0629 - accuracy: 0.9771 - val_loss: 0.2658 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "854/854 [==============================] - 13s 15ms/step - loss: 0.0658 - accuracy: 0.9744 - val_loss: 0.0839 - val_accuracy: 0.9698 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "854/854 [==============================] - 13s 15ms/step - loss: 0.0410 - accuracy: 0.9849 - val_loss: 0.0671 - val_accuracy: 0.9759 - lr: 5.0000e-04\n",
      "Epoch 8/100\n",
      "854/854 [==============================] - 13s 15ms/step - loss: 0.0332 - accuracy: 0.9884 - val_loss: 0.0418 - val_accuracy: 0.9848 - lr: 5.0000e-04\n",
      "Epoch 9/100\n",
      "854/854 [==============================] - 13s 15ms/step - loss: 0.0296 - accuracy: 0.9889 - val_loss: 0.0414 - val_accuracy: 0.9848 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "854/854 [==============================] - 13s 16ms/step - loss: 0.0284 - accuracy: 0.9900 - val_loss: 0.0496 - val_accuracy: 0.9824 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "854/854 [==============================] - 13s 15ms/step - loss: 0.0254 - accuracy: 0.9910 - val_loss: 0.0379 - val_accuracy: 0.9865 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "854/854 [==============================] - 13s 15ms/step - loss: 0.0264 - accuracy: 0.9913 - val_loss: 0.0439 - val_accuracy: 0.9842 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "854/854 [==============================] - 13s 15ms/step - loss: 0.0255 - accuracy: 0.9908 - val_loss: 0.0415 - val_accuracy: 0.9852 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "854/854 [==============================] - 13s 15ms/step - loss: 0.0224 - accuracy: 0.9922 - val_loss: 0.0550 - val_accuracy: 0.9823 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "854/854 [==============================] - 13s 15ms/step - loss: 0.0172 - accuracy: 0.9946 - val_loss: 0.0403 - val_accuracy: 0.9856 - lr: 2.5000e-04\n",
      "Epoch 16/100\n",
      "854/854 [==============================] - 13s 15ms/step - loss: 0.0149 - accuracy: 0.9948 - val_loss: 0.0423 - val_accuracy: 0.9853 - lr: 2.5000e-04\n",
      "547/547 [==============================] - 2s 3ms/step\n",
      "547/547 [==============================] - 1s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    bonafide     0.4468    0.9891    0.6156      3500\n",
      "       spoof     0.9961    0.6939    0.8180     14000\n",
      "\n",
      "    accuracy                         0.7529     17500\n",
      "   macro avg     0.7215    0.8415    0.7168     17500\n",
      "weighted avg     0.8862    0.7529    0.7775     17500\n",
      "\n",
      "Equal Error Rate (EER): 8.3464%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, roc_curve\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.metrics import roc_curve \n",
    "import sklearn.metrics  \n",
    "\n",
    "def load_and_preprocess_data(train_file, dev_file, test_file):\n",
    "    train = pd.read_csv(train_file)\n",
    "    dev = pd.read_csv(dev_file)\n",
    "    test = pd.read_csv(test_file)\n",
    "    \n",
    "    X_train = train.iloc[:, 1:-1].values\n",
    "    y_train = train['class'].values\n",
    "\n",
    "    X_dev = dev.iloc[:, 1:-1].values\n",
    "    y_dev = dev['class'].values\n",
    "\n",
    "    X_test = test.iloc[:, 1:-1].values\n",
    "    y_test = test['class'].values\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "    y_dev = encoder.transform(y_dev)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_dev = np.expand_dims(X_dev, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "    \n",
    "    return X_train, y_train, X_dev, y_dev, X_test, y_test\n",
    "\n",
    "def compute_eer(label, pred, positive_label=1):\n",
    "    fpr, tpr, threshold = sklearn.metrics.roc_curve(label, pred, pos_label=positive_label)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer = (eer_1 + eer_2) / 2\n",
    "    return eer\n",
    "    \n",
    "def build_cnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(), \n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),   \n",
    "        Conv1D(128, kernel_size=3, activation='relu'),\n",
    "        BatchNormalization(),  \n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),  \n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(2, activation='softmax') \n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_cnn(train_file, dev_file, test_file, epochs=10, batch_size=32):\n",
    "    \n",
    "    X_train, y_train, X_dev, y_dev, X_test, y_test = load_and_preprocess_data(train_file, dev_file, test_file)\n",
    "    \n",
    "    model = build_cnn_model(input_shape=(X_train.shape[1], 1))\n",
    "  \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=epochs, batch_size=batch_size,\n",
    "              callbacks=[reduce_lr, early_stop])\n",
    "    \n",
    "    y_pred_prob = model.predict(X_test)[:, 1]  \n",
    "    \n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    print(classification_report(y_test, y_pred, target_names=['bonafide', 'spoof'], digits=4))\n",
    "\n",
    "    eer = compute_eer(y_test, y_pred_prob, positive_label=1)\n",
    "    eer_percentage = eer * 100  \n",
    "    print(f'Equal Error Rate (EER): {eer_percentage:.4f}%')\n",
    "    \n",
    "    return model\n",
    "# Example usage:\n",
    "train_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/English_train_MERT-v1-330M/English_train_MERT-v1-330M.csv'\n",
    "dev_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/English_dev_MERT-v1-330M/English_dev_MERT-v1-330M.csv'\n",
    "test_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/English_test_MERT-v1-330M/English_test_MERT-v1-330M.csv'\n",
    "\n",
    "# Call the function to train and evaluate the CNN model\n",
    "cnn_model = train_and_evaluate_cnn(train_file, dev_file, test_file, epochs=100, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7259a3-5e97-4178-8931-bea66b4e69c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# C MERT v1 330M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d679dd81-e806-4c49-8e5e-205d79d74f59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 13:05:36.537963: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-25 13:05:36.711784: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-25 13:05:36.711876: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-25 13:05:36.713884: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-25 13:05:36.713940: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-25 13:05:36.713970: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-25 13:05:36.914363: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-25 13:05:36.914479: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-25 13:05:36.914493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-09-25 13:05:36.914562: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-25 13:05:36.915165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3620 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 6GB Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 13:05:38.878005: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-09-25 13:05:39.151362: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-09-25 13:05:39.858974: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-09-25 13:05:41.206470: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fc7146cc160 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-09-25 13:05:41.206553: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 6GB Laptop GPU, Compute Capability 8.6\n",
      "2024-09-25 13:05:41.238452: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727249741.400210    6368 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "854/854 [==============================] - 24s 22ms/step - loss: 0.2666 - accuracy: 0.8865 - val_loss: 0.1350 - val_accuracy: 0.9500 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "854/854 [==============================] - 17s 20ms/step - loss: 0.1004 - accuracy: 0.9605 - val_loss: 0.0667 - val_accuracy: 0.9799 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "854/854 [==============================] - 17s 20ms/step - loss: 0.0754 - accuracy: 0.9714 - val_loss: 0.0591 - val_accuracy: 0.9765 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "854/854 [==============================] - 17s 20ms/step - loss: 0.0619 - accuracy: 0.9774 - val_loss: 0.0810 - val_accuracy: 0.9685 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "854/854 [==============================] - 17s 20ms/step - loss: 0.0568 - accuracy: 0.9789 - val_loss: 0.0720 - val_accuracy: 0.9738 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "854/854 [==============================] - 18s 21ms/step - loss: 0.0499 - accuracy: 0.9822 - val_loss: 0.0429 - val_accuracy: 0.9856 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "854/854 [==============================] - 17s 20ms/step - loss: 0.0434 - accuracy: 0.9845 - val_loss: 0.0393 - val_accuracy: 0.9879 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "854/854 [==============================] - 17s 19ms/step - loss: 0.0390 - accuracy: 0.9861 - val_loss: 0.0602 - val_accuracy: 0.9785 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "854/854 [==============================] - 17s 20ms/step - loss: 0.0383 - accuracy: 0.9864 - val_loss: 0.0516 - val_accuracy: 0.9812 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "854/854 [==============================] - 17s 20ms/step - loss: 0.0403 - accuracy: 0.9860 - val_loss: 0.0343 - val_accuracy: 0.9887 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "854/854 [==============================] - 17s 20ms/step - loss: 0.0421 - accuracy: 0.9843 - val_loss: 0.0698 - val_accuracy: 0.9727 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "854/854 [==============================] - 17s 20ms/step - loss: 0.0346 - accuracy: 0.9885 - val_loss: 0.0476 - val_accuracy: 0.9830 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "854/854 [==============================] - 17s 20ms/step - loss: 0.0287 - accuracy: 0.9896 - val_loss: 0.0693 - val_accuracy: 0.9730 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "854/854 [==============================] - 20s 24ms/step - loss: 0.0218 - accuracy: 0.9927 - val_loss: 0.0338 - val_accuracy: 0.9887 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "854/854 [==============================] - 18s 22ms/step - loss: 0.0167 - accuracy: 0.9941 - val_loss: 0.0300 - val_accuracy: 0.9896 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "854/854 [==============================] - 17s 20ms/step - loss: 0.0169 - accuracy: 0.9938 - val_loss: 0.0508 - val_accuracy: 0.9811 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "854/854 [==============================] - 17s 19ms/step - loss: 0.0157 - accuracy: 0.9945 - val_loss: 0.0337 - val_accuracy: 0.9884 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "854/854 [==============================] - 17s 19ms/step - loss: 0.0129 - accuracy: 0.9957 - val_loss: 0.0428 - val_accuracy: 0.9838 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "854/854 [==============================] - 17s 19ms/step - loss: 0.0106 - accuracy: 0.9967 - val_loss: 0.0309 - val_accuracy: 0.9886 - lr: 2.5000e-04\n",
      "Epoch 20/100\n",
      "854/854 [==============================] - 17s 19ms/step - loss: 0.0097 - accuracy: 0.9971 - val_loss: 0.0393 - val_accuracy: 0.9855 - lr: 2.5000e-04\n",
      "547/547 [==============================] - 2s 4ms/step\n",
      "547/547 [==============================] - 2s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    bonafide     0.4588    0.9709    0.6231      3500\n",
      "       spoof     0.9899    0.7136    0.8294     14000\n",
      "\n",
      "    accuracy                         0.7651     17500\n",
      "   macro avg     0.7243    0.8422    0.7262     17500\n",
      "weighted avg     0.8837    0.7651    0.7881     17500\n",
      "\n",
      "Equal Error Rate (EER): 11.1857%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, roc_curve\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.metrics import roc_curve \n",
    "import sklearn.metrics  \n",
    "\n",
    "def load_and_preprocess_data(train_file, dev_file, test_file):\n",
    "    train = pd.read_csv(train_file)\n",
    "    dev = pd.read_csv(dev_file)\n",
    "    test = pd.read_csv(test_file)\n",
    "    \n",
    "    X_train = train.iloc[:, 1:-1].values\n",
    "    y_train = train['class'].values\n",
    "\n",
    "    X_dev = dev.iloc[:, 1:-1].values\n",
    "    y_dev = dev['class'].values\n",
    "\n",
    "    X_test = test.iloc[:, 1:-1].values\n",
    "    y_test = test['class'].values\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "    y_dev = encoder.transform(y_dev)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_dev = np.expand_dims(X_dev, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "    \n",
    "    return X_train, y_train, X_dev, y_dev, X_test, y_test\n",
    "\n",
    "def compute_eer(label, pred, positive_label=1):\n",
    "    fpr, tpr, threshold = sklearn.metrics.roc_curve(label, pred, pos_label=positive_label)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer = (eer_1 + eer_2) / 2\n",
    "    return eer\n",
    "    \n",
    "def build_cnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(), \n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),   \n",
    "        Conv1D(128, kernel_size=3, activation='relu'),\n",
    "        BatchNormalization(),  \n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),  \n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(2, activation='softmax') \n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_cnn(train_file, dev_file, test_file, epochs=10, batch_size=32):\n",
    "    \n",
    "    X_train, y_train, X_dev, y_dev, X_test, y_test = load_and_preprocess_data(train_file, dev_file, test_file)\n",
    "    \n",
    "    model = build_cnn_model(input_shape=(X_train.shape[1], 1))\n",
    "  \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=epochs, batch_size=batch_size,\n",
    "              callbacks=[reduce_lr, early_stop])\n",
    "    \n",
    "    y_pred_prob = model.predict(X_test)[:, 1]  \n",
    "    \n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    print(classification_report(y_test, y_pred, target_names=['bonafide', 'spoof'], digits=4))\n",
    "\n",
    "    eer = compute_eer(y_test, y_pred_prob, positive_label=1)\n",
    "    eer_percentage = eer * 100  \n",
    "    print(f'Equal Error Rate (EER): {eer_percentage:.4f}%')\n",
    "    \n",
    "    return model\n",
    "# Example usage:\n",
    "train_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/Chinese_train_MERT-v1-330M/Chinese_train_MERT-v1-330M.csv'\n",
    "dev_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/Chinese_dev_MERT-v1-330M/Chinese_dev_MERT-v1-330M.csv'\n",
    "test_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/Chinese_test_MERT-v1-330M/Chinese_test_MERT-v1-330M.csv'\n",
    "\n",
    "# Call the function to train and evaluate the CNN model\n",
    "cnn_model = train_and_evaluate_cnn(train_file, dev_file, test_file, epochs=100, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f516464-09bc-43f5-ac38-2171a207f1e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# C wav2vec2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3ce8ccc-5780-4241-bf91-47c93b365fee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-04 12:32:38.017531: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-04 12:32:38.212351: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-04 12:32:38.212425: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-04 12:32:38.216467: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-04 12:32:38.216531: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-04 12:32:38.216563: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-04 12:32:38.742244: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-04 12:32:38.742347: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-04 12:32:38.742362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-10-04 12:32:38.742425: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-04 12:32:38.742452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3620 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 6GB Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-04 12:32:39.940267: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-10-04 12:32:40.173632: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-10-04 12:32:40.541663: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-10-04 12:32:41.402259: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f65819ebd50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-10-04 12:32:41.402305: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 6GB Laptop GPU, Compute Capability 8.6\n",
      "2024-10-04 12:32:41.416496: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728025361.495617  319440 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "854/854 [==============================] - 15s 14ms/step - loss: 0.0301 - accuracy: 0.9902 - val_loss: 0.0082 - val_accuracy: 0.9978 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0085 - accuracy: 0.9974 - val_loss: 0.0069 - val_accuracy: 0.9979 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0150 - accuracy: 0.9951 - val_loss: 0.0054 - val_accuracy: 0.9982 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0090 - accuracy: 0.9971 - val_loss: 0.0027 - val_accuracy: 0.9993 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0088 - accuracy: 0.9971 - val_loss: 0.0064 - val_accuracy: 0.9985 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0079 - accuracy: 0.9977 - val_loss: 0.0022 - val_accuracy: 0.9996 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "854/854 [==============================] - 11s 12ms/step - loss: 0.0084 - accuracy: 0.9977 - val_loss: 0.0028 - val_accuracy: 0.9998 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0086 - accuracy: 0.9976 - val_loss: 0.0082 - val_accuracy: 0.9977 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0072 - accuracy: 0.9976 - val_loss: 0.0031 - val_accuracy: 0.9991 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "854/854 [==============================] - 11s 12ms/step - loss: 0.0030 - accuracy: 0.9990 - val_loss: 0.0025 - val_accuracy: 0.9991 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 0.0065 - val_accuracy: 0.9981 - lr: 5.0000e-04\n",
      "547/547 [==============================] - 1s 2ms/step\n",
      "547/547 [==============================] - 1s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    bonafide     0.4949    0.9983    0.6617      3500\n",
      "       spoof     0.9994    0.7453    0.8538     14000\n",
      "\n",
      "    accuracy                         0.7959     17500\n",
      "   macro avg     0.7472    0.8718    0.7578     17500\n",
      "weighted avg     0.8985    0.7959    0.8154     17500\n",
      "\n",
      "Equal Error Rate (EER): 4.1429%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, roc_curve\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.metrics import roc_curve \n",
    "import sklearn.metrics  \n",
    "\n",
    "def load_and_preprocess_data(train_file, dev_file, test_file):\n",
    "    train = pd.read_csv(train_file)\n",
    "    dev = pd.read_csv(dev_file)\n",
    "    test = pd.read_csv(test_file)\n",
    "    \n",
    "    X_train = train.iloc[:, 1:-1].values\n",
    "    y_train = train['class'].values\n",
    "\n",
    "    X_dev = dev.iloc[:, 1:-1].values\n",
    "    y_dev = dev['class'].values\n",
    "\n",
    "    X_test = test.iloc[:, 1:-1].values\n",
    "    y_test = test['class'].values\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "    y_dev = encoder.transform(y_dev)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_dev = np.expand_dims(X_dev, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "    \n",
    "    return X_train, y_train, X_dev, y_dev, X_test, y_test\n",
    "\n",
    "def compute_eer(label, pred, positive_label=1):\n",
    "    fpr, tpr, threshold = sklearn.metrics.roc_curve(label, pred, pos_label=positive_label)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer = (eer_1 + eer_2) / 2\n",
    "    return eer\n",
    "    \n",
    "def build_cnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(), \n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),   \n",
    "        Conv1D(128, kernel_size=3, activation='relu'),\n",
    "        BatchNormalization(),  \n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),  \n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(2, activation='softmax') \n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_cnn(train_file, dev_file, test_file, epochs=10, batch_size=32):\n",
    "    \n",
    "    X_train, y_train, X_dev, y_dev, X_test, y_test = load_and_preprocess_data(train_file, dev_file, test_file)\n",
    "    \n",
    "    model = build_cnn_model(input_shape=(X_train.shape[1], 1))\n",
    "  \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=epochs, batch_size=batch_size,\n",
    "              callbacks=[reduce_lr, early_stop])\n",
    "    \n",
    "    y_pred_prob = model.predict(X_test)[:, 1]  \n",
    "    \n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    print(classification_report(y_test, y_pred, target_names=['bonafide', 'spoof'], digits=4))\n",
    "\n",
    "    eer = compute_eer(y_test, y_pred_prob, positive_label=1)\n",
    "    eer_percentage = eer * 100  \n",
    "    print(f'Equal Error Rate (EER): {eer_percentage:.4f}%')\n",
    "    \n",
    "    return model\n",
    "# Example usage:\n",
    "train_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/Wav2Vec2/Chinese_train_Wav2Vec2.csv'\n",
    "dev_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/Wav2Vec2/Chinese_dev_Wav2Vec2.csv'\n",
    "test_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/Wav2Vec2/Chinese_test_Wav2Vec2.csv'\n",
    "\n",
    "# Call the function to train and evaluate the CNN model\n",
    "cnn_model = train_and_evaluate_cnn(train_file, dev_file, test_file, epochs=100, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1230f49-5496-49ef-95a4-5444f971da9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# E wav2vec2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a96304c-dc2f-45ef-b17f-cc451d78e5af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "854/854 [==============================] - 13s 14ms/step - loss: 0.0294 - accuracy: 0.9902 - val_loss: 0.0183 - val_accuracy: 0.9937 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "854/854 [==============================] - 12s 14ms/step - loss: 0.0141 - accuracy: 0.9960 - val_loss: 0.0025 - val_accuracy: 0.9991 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "854/854 [==============================] - 12s 14ms/step - loss: 0.0129 - accuracy: 0.9958 - val_loss: 9.1782e-04 - val_accuracy: 0.9998 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "854/854 [==============================] - 12s 14ms/step - loss: 0.0087 - accuracy: 0.9973 - val_loss: 0.0022 - val_accuracy: 0.9992 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0132 - accuracy: 0.9956 - val_loss: 0.0031 - val_accuracy: 0.9988 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0049 - accuracy: 0.9984 - val_loss: 5.6817e-04 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0037 - accuracy: 0.9990 - val_loss: 7.6448e-04 - val_accuracy: 0.9999 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "854/854 [==============================] - 11s 12ms/step - loss: 0.0092 - accuracy: 0.9967 - val_loss: 7.2305e-04 - val_accuracy: 0.9998 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0059 - accuracy: 0.9979 - val_loss: 0.0030 - val_accuracy: 0.9988 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 5.6726e-04 - val_accuracy: 0.9999 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0038 - accuracy: 0.9987 - val_loss: 0.0045 - val_accuracy: 0.9982 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0028 - accuracy: 0.9989 - val_loss: 4.5642e-04 - val_accuracy: 0.9999 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "854/854 [==============================] - 11s 12ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 1.5371e-04 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 1.8482e-04 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0035 - accuracy: 0.9989 - val_loss: 3.6043e-04 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0031 - accuracy: 0.9989 - val_loss: 0.0020 - val_accuracy: 0.9991 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 7.1198e-04 - val_accuracy: 0.9999 - lr: 2.5000e-04\n",
      "Epoch 18/100\n",
      "854/854 [==============================] - 10s 12ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 1.4097e-04 - val_accuracy: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 19/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0023 - accuracy: 0.9992 - val_loss: 6.9987e-05 - val_accuracy: 1.0000 - lr: 2.5000e-04\n",
      "Epoch 20/100\n",
      "854/854 [==============================] - 10s 12ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 1.7442e-04 - val_accuracy: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 21/100\n",
      "854/854 [==============================] - 10s 12ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 9.8235e-05 - val_accuracy: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 22/100\n",
      "854/854 [==============================] - 10s 12ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 1.2730e-04 - val_accuracy: 1.0000 - lr: 1.2500e-04\n",
      "Epoch 23/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 7.4875e-04 - accuracy: 0.9998 - val_loss: 1.6930e-04 - val_accuracy: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 24/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 9.1158e-04 - accuracy: 0.9997 - val_loss: 4.3802e-05 - val_accuracy: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 25/100\n",
      "854/854 [==============================] - 12s 14ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 1.1770e-04 - val_accuracy: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 26/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 6.6620e-04 - accuracy: 0.9999 - val_loss: 1.3047e-04 - val_accuracy: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 27/100\n",
      "854/854 [==============================] - 11s 12ms/step - loss: 5.9493e-04 - accuracy: 0.9999 - val_loss: 5.9403e-05 - val_accuracy: 1.0000 - lr: 6.2500e-05\n",
      "Epoch 28/100\n",
      "854/854 [==============================] - 10s 12ms/step - loss: 6.0273e-04 - accuracy: 0.9999 - val_loss: 1.5350e-04 - val_accuracy: 1.0000 - lr: 3.1250e-05\n",
      "Epoch 29/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 9.2821e-04 - accuracy: 0.9996 - val_loss: 1.8620e-04 - val_accuracy: 1.0000 - lr: 3.1250e-05\n",
      "547/547 [==============================] - 1s 2ms/step\n",
      "547/547 [==============================] - 1s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    bonafide     0.5528    1.0000    0.7120      3500\n",
      "       spoof     1.0000    0.7978    0.8875     14000\n",
      "\n",
      "    accuracy                         0.8382     17500\n",
      "   macro avg     0.7764    0.8989    0.7998     17500\n",
      "weighted avg     0.9106    0.8382    0.8524     17500\n",
      "\n",
      "Equal Error Rate (EER): 2.1429%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, roc_curve\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.metrics import roc_curve \n",
    "import sklearn.metrics  \n",
    "\n",
    "def load_and_preprocess_data(train_file, dev_file, test_file):\n",
    "    train = pd.read_csv(train_file)\n",
    "    dev = pd.read_csv(dev_file)\n",
    "    test = pd.read_csv(test_file)\n",
    "    \n",
    "    X_train = train.iloc[:, 1:-1].values\n",
    "    y_train = train['class'].values\n",
    "\n",
    "    X_dev = dev.iloc[:, 1:-1].values\n",
    "    y_dev = dev['class'].values\n",
    "\n",
    "    X_test = test.iloc[:, 1:-1].values\n",
    "    y_test = test['class'].values\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "    y_dev = encoder.transform(y_dev)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_dev = np.expand_dims(X_dev, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "    \n",
    "    return X_train, y_train, X_dev, y_dev, X_test, y_test\n",
    "\n",
    "def compute_eer(label, pred, positive_label=1):\n",
    "    fpr, tpr, threshold = sklearn.metrics.roc_curve(label, pred, pos_label=positive_label)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer = (eer_1 + eer_2) / 2\n",
    "    return eer\n",
    "    \n",
    "def build_cnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(), \n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),   \n",
    "        Conv1D(128, kernel_size=3, activation='relu'),\n",
    "        BatchNormalization(),  \n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),  \n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(2, activation='softmax') \n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_cnn(train_file, dev_file, test_file, epochs=10, batch_size=32):\n",
    "    \n",
    "    X_train, y_train, X_dev, y_dev, X_test, y_test = load_and_preprocess_data(train_file, dev_file, test_file)\n",
    "    \n",
    "    model = build_cnn_model(input_shape=(X_train.shape[1], 1))\n",
    "  \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=epochs, batch_size=batch_size,\n",
    "              callbacks=[reduce_lr, early_stop])\n",
    "    \n",
    "    y_pred_prob = model.predict(X_test)[:, 1]  \n",
    "    \n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    print(classification_report(y_test, y_pred, target_names=['bonafide', 'spoof'], digits=4))\n",
    "\n",
    "    eer = compute_eer(y_test, y_pred_prob, positive_label=1)\n",
    "    eer_percentage = eer * 100  \n",
    "    print(f'Equal Error Rate (EER): {eer_percentage:.4f}%')\n",
    "    \n",
    "    return model\n",
    "# Example usage:\n",
    "train_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/Wav2Vec2/English_train_Wav2Vec2.csv'\n",
    "dev_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/Wav2Vec2/English_dev_Wav2Vec2.csv'\n",
    "test_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/Wav2Vec2/English_test_Wav2Vec2.csv'\n",
    "\n",
    "# Call the function to train and evaluate the CNN model\n",
    "cnn_model = train_and_evaluate_cnn(train_file, dev_file, test_file, epochs=100, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3858d6d4-ce39-4385-9dfb-98f3d171d9db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# C unispeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "744c31e3-df50-4578-bed3-fb3b1a46fd55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "854/854 [==============================] - 12s 13ms/step - loss: 0.0498 - accuracy: 0.9820 - val_loss: 0.0317 - val_accuracy: 0.9911 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0287 - accuracy: 0.9896 - val_loss: 0.0284 - val_accuracy: 0.9910 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0277 - accuracy: 0.9904 - val_loss: 0.1083 - val_accuracy: 0.9559 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0207 - accuracy: 0.9926 - val_loss: 0.0659 - val_accuracy: 0.9738 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0202 - accuracy: 0.9933 - val_loss: 0.4511 - val_accuracy: 0.8762 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0157 - accuracy: 0.9945 - val_loss: 0.0143 - val_accuracy: 0.9957 - lr: 5.0000e-04\n",
      "Epoch 7/100\n",
      "854/854 [==============================] - 11s 12ms/step - loss: 0.0157 - accuracy: 0.9947 - val_loss: 0.0227 - val_accuracy: 0.9923 - lr: 5.0000e-04\n",
      "Epoch 8/100\n",
      "854/854 [==============================] - 11s 12ms/step - loss: 0.0135 - accuracy: 0.9956 - val_loss: 0.0251 - val_accuracy: 0.9915 - lr: 5.0000e-04\n",
      "Epoch 9/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0141 - accuracy: 0.9953 - val_loss: 0.0608 - val_accuracy: 0.9802 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "854/854 [==============================] - 12s 14ms/step - loss: 0.0150 - accuracy: 0.9953 - val_loss: 0.0183 - val_accuracy: 0.9954 - lr: 2.5000e-04\n",
      "Epoch 11/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0120 - accuracy: 0.9964 - val_loss: 0.0130 - val_accuracy: 0.9960 - lr: 2.5000e-04\n",
      "Epoch 12/100\n",
      "854/854 [==============================] - 12s 15ms/step - loss: 0.0110 - accuracy: 0.9965 - val_loss: 0.0125 - val_accuracy: 0.9957 - lr: 2.5000e-04\n",
      "Epoch 13/100\n",
      "854/854 [==============================] - 14s 17ms/step - loss: 0.0101 - accuracy: 0.9965 - val_loss: 0.0206 - val_accuracy: 0.9944 - lr: 2.5000e-04\n",
      "Epoch 14/100\n",
      "854/854 [==============================] - 14s 17ms/step - loss: 0.0106 - accuracy: 0.9970 - val_loss: 0.0126 - val_accuracy: 0.9966 - lr: 2.5000e-04\n",
      "Epoch 15/100\n",
      "854/854 [==============================] - 16s 18ms/step - loss: 0.0118 - accuracy: 0.9964 - val_loss: 0.0164 - val_accuracy: 0.9959 - lr: 2.5000e-04\n",
      "Epoch 16/100\n",
      "854/854 [==============================] - 15s 17ms/step - loss: 0.0090 - accuracy: 0.9969 - val_loss: 0.0510 - val_accuracy: 0.9836 - lr: 1.2500e-04\n",
      "Epoch 17/100\n",
      "854/854 [==============================] - 15s 17ms/step - loss: 0.0077 - accuracy: 0.9976 - val_loss: 0.0181 - val_accuracy: 0.9954 - lr: 1.2500e-04\n",
      "547/547 [==============================] - 2s 3ms/step\n",
      "547/547 [==============================] - 2s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    bonafide     0.5290    0.9866    0.6887      3500\n",
      "       spoof     0.9957    0.7804    0.8750     14000\n",
      "\n",
      "    accuracy                         0.8216     17500\n",
      "   macro avg     0.7623    0.8835    0.7818     17500\n",
      "weighted avg     0.9024    0.8216    0.8377     17500\n",
      "\n",
      "Equal Error Rate (EER): 8.6357%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, roc_curve\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.metrics import roc_curve \n",
    "import sklearn.metrics  \n",
    "\n",
    "def load_and_preprocess_data(train_file, dev_file, test_file):\n",
    "    train = pd.read_csv(train_file)\n",
    "    dev = pd.read_csv(dev_file)\n",
    "    test = pd.read_csv(test_file)\n",
    "    \n",
    "    X_train = train.iloc[:, 1:-1].values\n",
    "    y_train = train['class'].values\n",
    "\n",
    "    X_dev = dev.iloc[:, 1:-1].values\n",
    "    y_dev = dev['class'].values\n",
    "\n",
    "    X_test = test.iloc[:, 1:-1].values\n",
    "    y_test = test['class'].values\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "    y_dev = encoder.transform(y_dev)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_dev = np.expand_dims(X_dev, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "    \n",
    "    return X_train, y_train, X_dev, y_dev, X_test, y_test\n",
    "\n",
    "def compute_eer(label, pred, positive_label=1):\n",
    "    fpr, tpr, threshold = sklearn.metrics.roc_curve(label, pred, pos_label=positive_label)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer = (eer_1 + eer_2) / 2\n",
    "    return eer\n",
    "    \n",
    "def build_cnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(), \n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),   \n",
    "        Conv1D(128, kernel_size=3, activation='relu'),\n",
    "        BatchNormalization(),  \n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),  \n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(2, activation='softmax') \n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_cnn(train_file, dev_file, test_file, epochs=10, batch_size=32):\n",
    "    \n",
    "    X_train, y_train, X_dev, y_dev, X_test, y_test = load_and_preprocess_data(train_file, dev_file, test_file)\n",
    "    \n",
    "    model = build_cnn_model(input_shape=(X_train.shape[1], 1))\n",
    "  \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=epochs, batch_size=batch_size,\n",
    "              callbacks=[reduce_lr, early_stop])\n",
    "    \n",
    "    y_pred_prob = model.predict(X_test)[:, 1]  \n",
    "    \n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    print(classification_report(y_test, y_pred, target_names=['bonafide', 'spoof'], digits=4))\n",
    "\n",
    "    eer = compute_eer(y_test, y_pred_prob, positive_label=1)\n",
    "    eer_percentage = eer * 100  \n",
    "    print(f'Equal Error Rate (EER): {eer_percentage:.4f}%')\n",
    "    \n",
    "    return model\n",
    "\n",
    "train_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/UNISPEECH/Chinese_train_UniSpeechSAT/Chinese_train_UniSpeechSAT.csv'\n",
    "dev_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/UNISPEECH/Chinese_dev_UniSpeechSAT/Chinese_dev_UniSpeechSAT.csv'\n",
    "test_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/UNISPEECH/Chinese_test_UniSpeechSAT/Chinese_test_UniSpeechSAT.csv'\n",
    "\n",
    "\n",
    "cnn_model = train_and_evaluate_cnn(train_file, dev_file, test_file, epochs=100, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61826953-3398-4f5e-93fb-b66738f1e444",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# E Unispeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc24f733-195f-4f5a-9580-c3fe7a9864c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "854/854 [==============================] - 12s 13ms/step - loss: 0.0672 - accuracy: 0.9764 - val_loss: 0.0557 - val_accuracy: 0.9784 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0334 - accuracy: 0.9881 - val_loss: 0.0051 - val_accuracy: 0.9985 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0271 - accuracy: 0.9905 - val_loss: 0.0094 - val_accuracy: 0.9968 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0296 - accuracy: 0.9891 - val_loss: 0.0257 - val_accuracy: 0.9911 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0245 - accuracy: 0.9920 - val_loss: 0.0078 - val_accuracy: 0.9970 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0175 - accuracy: 0.9939 - val_loss: 0.0032 - val_accuracy: 0.9988 - lr: 5.0000e-04\n",
      "Epoch 7/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0156 - accuracy: 0.9950 - val_loss: 0.0142 - val_accuracy: 0.9946 - lr: 5.0000e-04\n",
      "Epoch 8/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0151 - accuracy: 0.9948 - val_loss: 0.0055 - val_accuracy: 0.9982 - lr: 5.0000e-04\n",
      "Epoch 9/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0128 - accuracy: 0.9956 - val_loss: 0.0010 - val_accuracy: 0.9998 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0147 - accuracy: 0.9949 - val_loss: 0.0012 - val_accuracy: 1.0000 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "854/854 [==============================] - 12s 14ms/step - loss: 0.0148 - accuracy: 0.9947 - val_loss: 0.0015 - val_accuracy: 0.9998 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "854/854 [==============================] - 12s 13ms/step - loss: 0.0181 - accuracy: 0.9937 - val_loss: 0.0264 - val_accuracy: 0.9902 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0159 - accuracy: 0.9948 - val_loss: 0.0024 - val_accuracy: 0.9995 - lr: 2.5000e-04\n",
      "Epoch 14/100\n",
      "854/854 [==============================] - 11s 13ms/step - loss: 0.0121 - accuracy: 0.9963 - val_loss: 0.0040 - val_accuracy: 0.9987 - lr: 2.5000e-04\n",
      "547/547 [==============================] - 1s 2ms/step\n",
      "547/547 [==============================] - 1s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    bonafide     0.6992    1.0000    0.8229      3500\n",
      "       spoof     1.0000    0.8924    0.9432     14000\n",
      "\n",
      "    accuracy                         0.9139     17500\n",
      "   macro avg     0.8496    0.9462    0.8831     17500\n",
      "weighted avg     0.9398    0.9139    0.9191     17500\n",
      "\n",
      "Equal Error Rate (EER): 2.0821%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, roc_curve\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.metrics import roc_curve \n",
    "import sklearn.metrics  \n",
    "\n",
    "def load_and_preprocess_data(train_file, dev_file, test_file):\n",
    "    train = pd.read_csv(train_file)\n",
    "    dev = pd.read_csv(dev_file)\n",
    "    test = pd.read_csv(test_file)\n",
    "    \n",
    "    X_train = train.iloc[:, 1:-1].values\n",
    "    y_train = train['class'].values\n",
    "\n",
    "    X_dev = dev.iloc[:, 1:-1].values\n",
    "    y_dev = dev['class'].values\n",
    "\n",
    "    X_test = test.iloc[:, 1:-1].values\n",
    "    y_test = test['class'].values\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "    y_dev = encoder.transform(y_dev)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_dev = np.expand_dims(X_dev, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "    \n",
    "    return X_train, y_train, X_dev, y_dev, X_test, y_test\n",
    "\n",
    "def compute_eer(label, pred, positive_label=1):\n",
    "    fpr, tpr, threshold = sklearn.metrics.roc_curve(label, pred, pos_label=positive_label)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer = (eer_1 + eer_2) / 2\n",
    "    return eer\n",
    "    \n",
    "def build_cnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(), \n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),   \n",
    "        Conv1D(128, kernel_size=3, activation='relu'),\n",
    "        BatchNormalization(),  \n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),  \n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(2, activation='softmax') \n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_cnn(train_file, dev_file, test_file, epochs=10, batch_size=32):\n",
    "    \n",
    "    X_train, y_train, X_dev, y_dev, X_test, y_test = load_and_preprocess_data(train_file, dev_file, test_file)\n",
    "    \n",
    "    model = build_cnn_model(input_shape=(X_train.shape[1], 1))\n",
    "  \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=epochs, batch_size=batch_size,\n",
    "              callbacks=[reduce_lr, early_stop])\n",
    "    \n",
    "    y_pred_prob = model.predict(X_test)[:, 1]  \n",
    "    \n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    print(classification_report(y_test, y_pred, target_names=['bonafide', 'spoof'], digits=4))\n",
    "\n",
    "    eer = compute_eer(y_test, y_pred_prob, positive_label=1)\n",
    "    eer_percentage = eer * 100  \n",
    "    print(f'Equal Error Rate (EER): {eer_percentage:.4f}%')\n",
    "    \n",
    "    return model\n",
    "\n",
    "train_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/UNISPEECH/English_train_UniSpeechSAT/English_train_UniSpeechSAT.csv'\n",
    "dev_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/UNISPEECH/English_dev_UniSpeechSAT/English_dev_UniSpeechSAT.csv'\n",
    "test_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/UNISPEECH/English_test_UniSpeechSAT/English_test_UniSpeechSAT.csv'\n",
    "\n",
    "\n",
    "cnn_model = train_and_evaluate_cnn(train_file, dev_file, test_file, epochs=100, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034f2d00-8029-4b42-bf91-70b9abdeb2c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# E whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "908a30a7-014a-4b85-82db-f9dfc2694702",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'class'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 96\u001b[0m\n\u001b[1;32m     92\u001b[0m dev_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/Whisper/English_dev_Whisper/English_dev_Whisper.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     93\u001b[0m test_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/Whisper/English_test_Whisper/English_test_Whisper.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 96\u001b[0m cnn_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_cnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 70\u001b[0m, in \u001b[0;36mtrain_and_evaluate_cnn\u001b[0;34m(train_file, dev_file, test_file, epochs, batch_size)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_and_evaluate_cnn\u001b[39m(train_file, dev_file, test_file, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m):\n\u001b[0;32m---> 70\u001b[0m     X_train, y_train, X_dev, y_dev, X_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_preprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     model \u001b[38;5;241m=\u001b[39m build_cnn_model(input_shape\u001b[38;5;241m=\u001b[39m(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     74\u001b[0m     reduce_lr \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m, in \u001b[0;36mload_and_preprocess_data\u001b[0;34m(train_file, dev_file, test_file)\u001b[0m\n\u001b[1;32m     15\u001b[0m test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(test_file)\n\u001b[1;32m     17\u001b[0m X_train \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m---> 18\u001b[0m y_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     20\u001b[0m X_dev \u001b[38;5;241m=\u001b[39m dev\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     21\u001b[0m y_dev \u001b[38;5;241m=\u001b[39m dev[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'class'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, roc_curve\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.metrics import roc_curve \n",
    "import sklearn.metrics  \n",
    "\n",
    "def load_and_preprocess_data(train_file, dev_file, test_file):\n",
    "    train = pd.read_csv(train_file)\n",
    "    dev = pd.read_csv(dev_file)\n",
    "    test = pd.read_csv(test_file)\n",
    "    \n",
    "    X_train = train.iloc[:, 1:-1].values\n",
    "    y_train = train['class'].values\n",
    "\n",
    "    X_dev = dev.iloc[:, 1:-1].values\n",
    "    y_dev = dev['class'].values\n",
    "\n",
    "    X_test = test.iloc[:, 1:-1].values\n",
    "    y_test = test['class'].values\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    y_train = encoder.fit_transform(y_train)\n",
    "    y_dev = encoder.transform(y_dev)\n",
    "    y_test = encoder.transform(y_test)\n",
    "\n",
    "    X_train = np.expand_dims(X_train, axis=2)\n",
    "    X_dev = np.expand_dims(X_dev, axis=2)\n",
    "    X_test = np.expand_dims(X_test, axis=2)\n",
    "    \n",
    "    return X_train, y_train, X_dev, y_dev, X_test, y_test\n",
    "\n",
    "def compute_eer(label, pred, positive_label=1):\n",
    "    fpr, tpr, threshold = sklearn.metrics.roc_curve(label, pred, pos_label=positive_label)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_1 = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer_2 = fnr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "    eer = (eer_1 + eer_2) / 2\n",
    "    return eer\n",
    "    \n",
    "def build_cnn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(), \n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),   \n",
    "        Conv1D(128, kernel_size=3, activation='relu'),\n",
    "        BatchNormalization(),  \n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),  \n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(2, activation='softmax') \n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_cnn(train_file, dev_file, test_file, epochs=10, batch_size=32):\n",
    "    \n",
    "    X_train, y_train, X_dev, y_dev, X_test, y_test = load_and_preprocess_data(train_file, dev_file, test_file)\n",
    "    \n",
    "    model = build_cnn_model(input_shape=(X_train.shape[1], 1))\n",
    "  \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=epochs, batch_size=batch_size,\n",
    "              callbacks=[reduce_lr, early_stop])\n",
    "    \n",
    "    y_pred_prob = model.predict(X_test)[:, 1]  \n",
    "    \n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    print(classification_report(y_test, y_pred, target_names=['bonafide', 'spoof'], digits=4))\n",
    "\n",
    "    eer = compute_eer(y_test, y_pred_prob, positive_label=1)\n",
    "    eer_percentage = eer * 100  \n",
    "    print(f'Equal Error Rate (EER): {eer_percentage:.4f}%')\n",
    "    \n",
    "    return model\n",
    "\n",
    "train_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/Whisper/English_train_Whisper/English_train_Whisper.csv'\n",
    "dev_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/Whisper/English_dev_Whisper/English_dev_Whisper.csv'\n",
    "test_file = '/home/girish/Girish/RESEARCH/EMO-FAKE/FEATURES/Whisper/English_test_Whisper/English_test_Whisper.csv'\n",
    "\n",
    "\n",
    "cnn_model = train_and_evaluate_cnn(train_file, dev_file, test_file, epochs=100, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ee46a4a-4696-4d96-a2f0-692d261bbe1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling-Copy1.ipynb  Modeling.ipynb  TEST.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b7e282b-25f3-42c8-83ba-9342215633bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9969c42b-9695-4c3e-9a37-65bd952c5437",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=np.load('../FEATURES/ecg_mms.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b86fe334-5448-4746-a284-75cf17495245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16492, 1280)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5f0005-a436-4121-b462-aecd5a6e2a58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
